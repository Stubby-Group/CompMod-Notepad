---
title: "W2_Notes"
output: html_document
---
# Definitions and things to remember:
 - Alpha, Beta, and Sigma are PARAMETERS (unobserved variables). 
 - PRIORS are theand the distributions of the PARAMETERS (pp.94).


Model definition with corresponding R-code (For the height/weight example in chapter 4):
 -  hi ∼ Normal(μi, σ)          height ~ dnorm(mu,sigma) 
 -  μi = α + β(xi −  ̄x)         mu <- a + b*(weight-xbar)
 -  α ∼ Normal(178, 20)         a ~ dnorm(178,20)
 -  β ∼ Log-Normal(0, 1)        b ~ dlnorm(0,1) 
 -  σ ∼ Uniform(0, 50)          sigma ~ dunif(0,50)




```{r setup, include=FALSE}
library(rethinking)

```

### Chapter 3 ###

```{r Chapter 3}
# Suppose there is a blood test that correctly detects vampirism 95% of the time. In more precise and mathematical notation, Pr(positive test result|vampire) = 0.95. One percent of the time, it incorrectly diagnoses normal people as vampires, Pr(positive test result|mortal) = 0.01. The final bit of information we are told is that vampires are rather rare, being only 0.1% of the population, implying Pr(vampire) = 0.001. 

#Suppose now that someone tests positive for vampirism. What’s the probability that he or she is a bloodsucking immortal?

Pr_Positive_Vampire <- 0.95
Pr_Positive_Mortal <- 0.01
Pr_Vampire <- 0.001
Pr_Positive <- Pr_Positive_Vampire * Pr_Vampire +
  Pr_Positive_Mortal * ( 1 - Pr_Vampire )
Pr_Vampire_Positive <- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive
Pr_Vampire_Positive #That corresponds to an 8.7% chance that the suspect is actually a vampire.



# Better way to put it (you can try to "translate tasks like this, if you don't understand them): 
  # (1) In a population of 100,000 people, 100 of them are vampires.
  # (2) Of the 100 who are vampires, 95 of them will test positive for vampirism. 
  # (3) Of the 99,900 mortals, 999 of them will test positive for vampirism.
```

```{r 3.1. Sampling a grid-approximate posterior}
# Before beginning to work with samples, we need to generate them. Here’s a reminder for how to compute the posterior for the globe tossing model, using grid approximation. Remember, the posterior here means the probability of p conditional on the data.
p_grid <- seq( from=0 , to=1 , length.out=1000 ) 
prob_p <- rep( 1 , 1000 )
prob_data <- dbinom( 6 , size=9 , prob=p_grid ) 
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)

# Now we wish to draw 10,000 samples from this posterior. The individual values of p will appear in our samples in proportion to the posterior plausibility of each value.
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )

plot(samples) #10000 random samples are shown sequentially

dens(samples) 

# Posterior distribution
```

```{r 3.2. Sampling to summarize}
# Common questions (depending on purpose) include:
# • How much posterior probability lies below some parameter value?
# • How much posterior probability lies between two parameter values?
# • Which parameter value marks the lower 5% of the posterior probability?
# • Which range of parameter values contains 90% of the posterior probability? 
# • Which parameter value has highest posterior probability?

# These simple questions can be usefully divided into questions about 
#  (1) intervals of defined boundaries, 
#  (2) questions about intervals of defined probability mass, and 
#  (3) questions about point estimates.



### (1) intervals of defined boundaries ###
# What is the posterior probability that the proportion of water is less than 0.5?
sum(posterior[ p_grid < 0.5 ]) # add up posterior probability where p < 0.5
# or: find the frequency of parameter values below 0.5:
sum(samples < 0.5) / 10000 # Diff result die to diff samples

# how much posterior probability lies between 0.5 and 0.75:
sum( samples > 0.5 & samples < 0.75 ) / 1e4
 


### (2) intervals of defined mass ### 
# Confidence / credibility / compatibility intervals. What the interval indicates is a range of parameter values compatible with the model and data.
#Suppose for example you want to know the boundaries of the lower 80% posterior probabil- ity. You know this interval starts at p = 0. To find out where it stops, think of the samples as data and ask where the 80th percentile lies:
quantile(samples, 0.8)
# Similarly, the middle 80% interval lies between the 10th percentile and the 90th percentile.
quantile( samples , c( 0.1 , 0.9 ) )

#Percentile intervals (!!! I don't get it)
p_grid <- seq( from=0 , to=1 , length.out=1000 ) 
prior <- rep(1,1000)
likelihood <- dbinom( 3 , size=3 , prob=p_grid ) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )
PI( samples , prob=0.5 )
# HPDI = Highest posterior density interval = the narrowest interval containing the specified probability mass. HPDI and PI are very similar.
HPDI( samples , prob=0.5 )


### (3) Point estimate ### see the book
```

```{r 3.3.1 Sampling to simulate prediction - Dummy data}

###### 3.3.1: Dummy data (simulated predictions) ######

# Suppose N = 2, two tosses of the globe. Then there are only three possible observations: 0 water, 1 water, 2 water. You can quickly compute the probability of each, for any given value of p. Let’s use p = 0.7, which is just about the true proportion of water on the Earth:
dbinom( 0:2 , size=2 , prob=0.7 ) #This means that there’s a 9% chance of observing w = 0, a 42% chance of w = 1, and a 49% chance of w = 2.

#Now we’re going to simulate observations, using these probabilities. A single dummy data observation of W can be sampled with:
rbinom( 1 , size=2 , prob=0.7 ) #That 1 means “1 water in 2 tosses.” The “r” in rbinom stands for “random.” 
#It can alsogenerate more than one simulation at a time. A set of 10 simulations can be made by:
rbinom( 10 , size=2 , prob=0.7 )
#Let’s generate 100,000 dummy observations, just to verify that each value (0, 1, or 2) appears in proportion to its likelihood:
dummy_w <- rbinom( 1e5 , size=2 , prob=0.7 ) 
table(dummy_w)/1e5

# Only two tosses of the globe isn’t much of a sample, though. So now let’s simulate the same sample size as before, 9 tosses.
dummy_w <- rbinom( 1e5 , size=9 , prob=0.7 ) 
simplehist( dummy_w , xlab="dummy water count" )
```

```{r 3.3.2 Sampling to simulate prediction - Model checking}
# Ensuring that the model fitting worked correctly and evaluating the adequacy of a model for some purpose.


# To simulate predicted observations for a single value of p, say p = 0.6, you can use rbinom to generate random binomial samples:
 w <- rbinom( 1e4 , size=9 , prob=0.6 ) #This generates 10,000 (1e4) simulated predictions of 9 globe tosses (size=9), assuming p = 0.6.
simplehist(w)

#All you need to propagate parameter uncertainty into these predictions is replace the value 0.6 with samples from the posterior:
 w <- rbinom( 1e4 , size=9 , prob=samples )
```

### Chapter 4 ###

```{r 4.1. Why normal distributions are normal}
### 4.1.1. Normal by addition
# Football field / coint toss with 1,000 friends example: "To simulate this, we generate for each person a list of 16 random numbers between −1 and 1. These are the individual steps. Then we add these steps together to get the position after 16 steps. Then we need to replicate this procedure 1000 times."
pos <- replicate( 1000 , sum( runif(16,-1,1) ) )

hist(pos)
plot(density(pos))



### 4.1.2. Normal by multiplication
#we can sample a random growth rate for this example with this line of code:
prod( 1 + runif(12,0,0.1) ) # This code just samples 12 random numbers between 1.0 and 1.1, each representing a pro- portional increase in growth. Thus 1.0 means no additional growth and 1.1 means a 10% increase. The product of all 12 is computed and returned as output. Now what distribution do you think these random products will take? Let’s generate 10,000 of them and see:
growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) ) 
dens( growth , norm.comp=TRUE )

#small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions.
big <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) ) 
small <- replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )
dens( big , norm.comp=TRUE )
dens( small , norm.comp=TRUE )



### 4.1.3. Normal by log-multiplication
#Large deviates that are multi- plied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale. For example:
log.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )
dens(log.big, norm.comp=TRUE )
```

## OBS, you skipped pages 75-76, because they are boring and irrelevant. Might wanna go back there someday.

```{r 4.2. A language for describing models}
## 4.2.1. Re-describing the globe tossing model
#   W = observed count of water
#   N = the total number of tosses
#   p = the proportion of water on the globe
#The count W is distributed binomially with sample size N and probability p (this line defines the likelihood function). The prior for p is assumed to be uniform between zero and one (this line defines the priors).



#Okay... nothing actually happens here, sooo
```

```{r 4.3.1. Gaussian model of height}
# Building a linear regression model (kinda). Mean and standard deviation describe the distribution's shape.

# Loading example data:
data(Howell1)
d <- Howell1

# Summarizing data:
precis( d ) #If you cannot see the histograms on your system, use instead: precis(d,hist=FALSE)

# All we want for now are heights of adults in the sample:
 d2 <- d[ d$age >= 18 , ]
 
 dens(d2$height)
```

```{r 4.3.2. Gaussian model of height: The model}

#    h_i ∼ Normal(μ, σ)         [likelihood]
#    μ ∼ Normal(178, 20)        [μ prior]
#    σ ∼ Uniform(0, 50)         [σ prior]
#The prior for μ is a broad Gaussian prior, centered on 178 cm, with 95% of probability between 178 ± 40 cm. Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218 cm encom- passes a huge range of plausible mean heights for human populations. So domain-specific information has gone into this prior. Everyone knows something about human height and can set a reasonable and vague prior of this kind.

#plot priors:
curve( dnorm( x , 178 , 20 ) , from=100 , to=250 )

#The σ prior is a truly flat prior, a uniform one, that functions just to constrain σ to have positive probability between zero and 50 cm:
curve( dunif( x , 0 , 50 ) , from=-10 , to=60 )



#it’ll help to see what these priors imply about the distribution of individual heights. The prior predictive simulation is an essential part of your modeling. (...) Every posterior is also potentially a prior for a subsequent analysis, so you can process priors just like posteriors.
sample_mu <- rnorm( 1e4 , 178 , 20 )
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) 
dens( prior_h )

#Let’s use simulation again to see the implied heights:
sample_mu <- rnorm( 1e4 , 178 , 100 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) 
dens( prior_h )
```

```{r 4.3.3. Gaussian model of height: Grid approximation of the posterior distribution}
# The following is manual and is shown for conceptual rather than practical reasons.
mu.list <- seq( from=150, to=160 , length.out=100 ) 
sigma.list <- seq( from=7 , to=9 , length.out=100 ) 
post <- expand.grid( mu=mu.list , sigma=sigma.list ) 
post$LL <- sapply( 1:nrow(post) , function(i) sum(
  dnorm( d2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) ) 
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) + 
  dunif( post$sigma , 0 , 50 , TRUE ) 
post$prob <- exp( post$prod - max(post$prod) )

#Contour plot:
contour_xyz( post$mu , post$sigma , post$prob )

#Heat map:
image_xyz( post$mu , post$sigma , post$prob )
```

```{r 4.3.4. Gaussian model of height: Sampling from the posterior}
# since there are two parameters, and we want to sample combinations of them, we first randomly sample row numbers in post in proportion to the values in post$prob. Then we pull out the parameter values on those randomly sampled rows:
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE , prob=post$prob )
sample.mu <- post$mu[ sample.rows ] 
sample.sigma <- post$sigma[ sample.rows ]

plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )

#Now that you have these samples, you can describe the distribution of confidence in each combination of μ and σ by summarizing the samples.
#For example, to characterize the shapes of the marginal (“marginal” here means “averaging over the other parameters.”) posterior densities of μ and σ, all we need to do is:
dens( sample.mu ) 
dens( sample.sigma )
# To summarize the widths of these densities with posterior compatibility intervals:
PI( sample.mu ) 
PI( sample.sigma )

#Or:
mean()
median()
quantile()



# Overthinking: Repeating with only a fraction of original data (compressed code):
d3 <- sample( d2$height , size=20 )

mu.list <- seq( from=150, to=170 , length.out=200 ) 
sigma.list <- seq( from=4 , to=20 , length.out=200 ) 
post2 <- expand.grid( mu=mu.list , sigma=sigma.list ) 
post2$LL <- sapply( 1:nrow(post2) , function(i)
  sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] , log=TRUE ) ) )
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
  dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob <- exp( post2$prod - max(post2$prod) )
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob )
sample2.mu <- post2$mu[ sample2.rows ] 
sample2.sigma <- post2$sigma[ sample2.rows ] 
plot( sample2.mu , sample2.sigma , cex=0.5 , 
      col=col.alpha(rangi2,0.1) , 
      xlab="mu" , ylab="sigma" , pch=16 )

# You should also inspect the marginal posterior density for σ, averaging over μ, produced with:
dens( sample2.sigma , norm.comp=TRUE )
```

```{r 4.3.5. Gaussian model of height: Finding the posterior distribution with quap}
#Quadratic approximation
# quap-function: "The quap function works by using the model definition you were introduced to ear- lier in this chapter. Each line in the definition has a corresponding definition in the form of R code. The engine inside quap then uses these definitions to define the posterior probability at each combination of parameter values. Then it can climb the posterior distribution and find the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce an approximation of the posterior distribution"

#(Re)loading example data:
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]

# List of (something)
flist <- alist(
  height ~ dnorm( mu , sigma ),
  mu ~ dnorm( 178 , 20 ), 
  sigma ~ dunif( 0 , 50 )
)

#Fit the model to the data in the data frame d2 with:
m4.1 <- quap( flist , data=d2 )

#Posterior distribution:
precis( m4.1 )

# Interpretation: "This means the plausibility of each value of μ, after averaging over the plausibilities of each value of σ, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4. The 5.5% and 94.5% quantiles are percentile interval boundaries, corresponding to an 89% compatibility interval. Why 89%? It’s just the default. It displays a quite wide interval, so it shows a high-probability range of parameter values. If you want another interval, such as the conventional and mindless 95%, you can use precis(m4.1,prob=0.95). But I don’t recommend 95% intervals, because readers will have a hard time not viewing them as significance tests."
```

```{r 4.3.6. Gaussian model of height: Sampling from a quap}
#Variance-covariance matrix: A list of means and a matrix of variances and covariances are sufficient to describe a multi-dimensional Gaussian distribution. To see this matrix of variances and covariances, for model m4.1, use:
vcov( m4.1 ) #it tells us how each parameter relates to every other param- eter in the posterior distribution.

#A variance-covariance matrix can be factored into two elements: (1) a vector of variances for the parameters and (2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others:
diag( vcov( m4.1 ) ) #List of variances
cov2cor( vcov( m4.1 ) ) # Correlation matrix. Interpretation: The entries that aren't 1 are very close to zero in this (typical) example. This indicates that learning μ tells us nothing about σ and likewise that learning σ tells us nothing about μ.

# Sampeling:
post <- extract.samples( m4.1 , n=1e4 )
head(post) #Each value is a sample from the posterior, so the mean and standard deviation of each column will be very close to the MAP values from before. You can confirm this by summarizing the samples:
precis(post)

#Interpretation: Compare these values to the output from precis(m4.1). And you can use plot(post) to see how much they resemble the samples from the grid approximation in Figure 4.4 (page 86).

```

```{r 4.4. Linear prediction}
#plot adult height and weight against one another:
data(Howell1); d <- Howell1; d2 <- d[ d$age >= 18 , ] 
plot( d2$height ~ d2$weight )
```

```{r 4.4.1. Linear prediction: The linear model strategy}
# About the prior of Beta: Simulating the prior predictive distribution

#The goal is to simulate heights from the model, using only the priors. First, let’s consider a range of weight values to simulate over. The range of observed weights will do fine. Then we need to simulate a bunch of lines, the lines implied by the priors for α and β:
set.seed(2971)
N <- 100 # 100 lines 
a <- rnorm( N , 178 , 20 )
b <- rnorm( N , 0 , 10 )
#Now we have 100 pairs of α and β values. Now to plot the lines:
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , 
      xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar), 
                        from=min(d2$weight) , to=max(d2$weight) , add=TRUE, 
                        col=col.alpha("black",0.2) )
# Interpretation: "The pattern doesn’t look like any human population at all. It essentially says that the relationship between weight and height could be absurdly positive or negative. Before we’ve even seen the data, this is a bad model.

# Here is how to do better: We know that average height increases with average weight, at least up to a point. Let’s try restricting it to positive values. The easiest way to do this is to define the prior as Log-Normal instead. Defining β as Log-Normal(0,1) means to claim that the logarithm of β has a Normal(0,1) distribution.
b <- rlnorm( 1e4 , 0 , 1 )
dens( b , xlim=c(0,5) , adj=0.1 )
#Do the prior predictive simulation again, now with the Log-Normal prior:
set.seed(2971)
N <- 100 # 100 lines 
a <- rnorm( N , 178 , 20 )
b <- rlnorm( N , 0 , 1 )
#Now try to plot again. This is much more sensible. There is still a rare impossible relationship. But nearly all lines in the joint prior for α and β are now within human reason.
```

```{r 4.4.2. Linear prediction: Finding the posterior distribution}
# See the top of this rmd "Model definition with corresponding R-code". This info allows us to build a posterior approximation:
data(Howell1); d <- Howell1; d2 <- d[ d$age >= 18 , ] # loading data again, since it's a long way back library(rethinking)
xbar <- mean(d2$weight) # defining the average weight, x-bar 
# fitting model:
m4.3 <- quap( 
  alist(
    height ~ dnorm( mu , sigma ) , 
    mu <- a + b*( weight - xbar ) , 
    a ~ dnorm( 178 , 20 ) ,
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 ) 
    ) , data=d2 ) 

```

```{r 4.4.3. Linear prediction: Interpreting the posterior distribution}
#### Table of marginal distributions:
 precis( m4.3 ) # interpretation (of beta): "Since β is a slope, the value 0.90 can be read as a person 1 kg heavier is expected to be 0.90 cm taller. 89% of the posterior probability lies between 0.84 and 0.97. That suggests that β values close to zero or greatly above one are highly incompatible with these data and this model. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones"
round( vcov( m4.3 ) , 3 ) #Viewing the covariances among the parameters. Very little covariation among the parameters in this case.
pairs(m4.3) # shows both the marginal posteriors and the covariance


# Plotting posterior inference against the data:
#We’ll start with just the raw data and a single line. The code below plots the raw data, computes the posterior mean values for a and b, then draws the implied line:
plot( height ~ weight , data=d2 , col=rangi2 ) 
post <- extract.samples( m4.3 )
a_map <- mean(post$a)
b_map <- mean(post$b)
curve( a_map + b_map*(x - xbar) , add=TRUE ) 
#Each point in this plot is a single individual. The black line is defined by the mean slope β and mean intercept α.






#### Adding uncertainty around the mean:
#Looking closer at some of the samples:
post <- extract.samples( m4.3 ) 
post[1:5,] #Each row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by vcov(m4.3). The paired values of a and b on each row define a line. The average of very many of these lines is the posterior mean line. But the scatter around that average is meaningful, because it alters our confidence in the relationship between the predictor and the outcome.

#The following code extracts the first 10 cases and re-estimates the model:
N <- 10
dN <- d2[ 1:N , ] 
mN <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*( weight - mean(weight) ) , a ~ dnorm( 178 , 20 ) ,
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
    ) , data=dN )

#Now let’s plot 20 of these lines, to see what the uncertainty looks like.
 # extract 20 samples from the posterior 
post <- extract.samples( mN , n=20 ) 
 # display raw data and sample size
plot( dN$weight , dN$height ,
    xlim=range(d2$weight) , ylim=range(d2$height) ,
    col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N)) 
 # plot the lines, with transparency (looping over all 20 lines, using curve to display each)
for ( i in 1:20 )
  curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) , col=col.alpha("black",0.3)
         , add=TRUE ) 





#### Plotting regression intervals and contours:
#It’s more common, and often much clearer (than doing as we see above), to see the uncertainty displayed by plotting an interval or contour around the average regression line.
#Focus for the moment on a single weight value, say 50 kilograms. You can quickly make a list of 10,000 values of μ for an individual who weighs 50 kilograms, by using your samples from the posterior:
post <- extract.samples( m4.3 )
mu_at_50 <- post$a + post$b * ( 50 - xbar )
#plot the density for this vector of means:
dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )
#To find the 89% compatibility interval of μ at 50 kg, just use the PI command as usual:
 PI( mu_at_50 , prob=0.89 ) #Interpretation: What these numbers mean is that the central 89% of the ways for the model to produce the data place the average height between about 159 cm and 160 cm (conditional on the model and data), assuming the weight is 50 kg.
 
#we need to repeat the above calculation for every weight value on the horizontal axis, not just when it is 50 kg. We want to draw 89% intervals around the average slope.
mu <- link( m4.3 ) #What link will do is take your quap approximation, sample from the posterior distribution, and then compute μ for each case in the data and sample from the posterior distribution.
str(mu) #Each row is a sample from the posterior distribu- tion. The default is 1000 samples, but you can use as many or as few as you like.

#We actually want something slightly different: a distribution of μ for each unique weight value on the horizontal axis.
  # define sequence of weights to compute predictions for these values will be on the horizontal axis 
weight.seq <- seq( from=25 , to=70 , by=1 )
  # use link to compute mu for each sample from posterior and for each weight in weight.seq
mu <- link( m4.3 , data=data.frame(weight=weight.seq) ) 
str(mu) #And now there are only 46 columns in mu, because we fed it 46 different values for weight.

#Plot:
plot( height ~ weight , d2 , type="n" ) # use type="n" to hide raw data
for ( i in 1:100 )
  points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) ) # loop over samples and plot each mu value

# Finally: summarize the distribution for each weight value:
mu.mean <- apply( mu , 2 , mean ) #Read "apply(mu,2,mean)" as "compute the mean of each column (dimension “2”) of the matrix mu".
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
 

# IMPORTANT: You can plot these summaries on top of the data with a few lines of R code:
# plot raw data fading out points to make line and interval more visible 
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )
# plot the MAP line, aka the mean mu for each weight 
lines( weight.seq , mu.mean )
# plot a shaded region for 89% PI 
shade( mu.PI , weight.seq )



#See page 107 for summary of the process.
```

```{r 4.4.3.5 Linear prediction: Interpreting the posterior distribution: Prediction intervals}
# See book (p.108), idk what this is
sim.height <- sim( m4.3 , data=list(weight=weight.seq) ) 
str(sim.height) #matrix contains simulated heights, not distributions of plausible average height, μ.
#Summarizing
height.PI <- apply( sim.height , 2 , PI , prob=0.89 ) #height.PI contains the 89% posterior prediction interval of observable (according to the model) heights, across the values of weight in weight.seq.

#Let’s plot everything we’ve built up: (1) the average line, (2) the shaded region of 89% plausible μ, and (3) the boundaries of the simulated heights the model expects.
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) ) # plotting raw data
lines( weight.seq , mu.mean ) # drawing MAP line
shade( mu.HPDI , weight.seq ) # drawing HPDI region for line 
shade( height.PI , weight.seq ) # drawing PI region for simulated heights 
# See p. 109 for smooth shade outlines

```



```{r 4.5.1. Curves from lines: Polynomial regression}
data(Howell1)
d <- Howell1

plot( height ~ weight , d ) #2nd order polynomial

# Step one is standardizing


# Next, I’ll also build the square of weight_s as a separate variable:
d$weight_s <- ( d$weight - mean(d$weight) )/sd(d$weight) 
d$weight_s2 <- d$weight_s^2
m4.5 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b1*weight_s + b2*weight_s2 , a ~ dnorm( 178 , 20 ) ,
    b1 ~ dlnorm( 0 , 1 ) ,
    b2 ~ dnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
    ) , data=d )

precis( m4.5 ) #Interpretation: The parameter α (a) is still the intercept, so it tells us the expected value of height when weight is at its mean value. But it is no longer equal to the mean height in the sample, since there is no guarantee it should build a poly regression. And those β1 and β2 parameters are the linear and square components of the curve.

#We’ll calculate the mean relationship and the 89% intervals of the mean and the predictions, like in the previous section.
weight.seq <- seq( from=-2.2 , to=2 , length.out=30 )
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 ) 
mu <- link( m4.5 , data=pred_dat )
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( m4.5 , data=pred_dat )
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

#Now, plot:
plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) ) 
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )
shade( height.PI , weight.seq )

```

```{r 4.5.2. Curves from lines: Splines}
#In statistics, a spline is a smooth function built out of smaller, component functions.
#B-spline (B stands for basis)

#To see how B-splines work, we’ll need an example that is much wigglier—that’s a scien- tific term—than the !Kung stature data. Cherry trees blossom all over Japan in the spring each year, and the tradition of flower viewing (Hanami 花見) follows. The timing of the blossoms can vary a lot by year and century. Let’s load a thousand years of blossom dates:
data(cherry_blossoms) 
d <- cherry_blossoms 
precis(d)


```



### Chapter 5 ###
#The many variables and the spurious waffles

- Multiple regression: Using more than one predictor variable to model an outcome.
- Correlation and causation (& confounds).
- Interactions.
- Causal inference (how to degign and interpret regression models.


```{r 5.1. Spurious association}
# Example: Correlation between divorce rate and marriage rate. Does marriage cause divorce?

# load data and copy 
library(rethinking) 
data(WaffleDivorce) 
d <- WaffleDivorce
# standardize variables
d$D <- standardize( d$Divorce )
d$M <- standardize( d$Marriage )
d$A <- standardize( d$MedianAgeMarriage )

#plotting age and marriage: Simulating (posterior) data from the priors (I think):
m5.1 <- quap( 
  alist(
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bA * A ,
    a ~ dnorm( 0 , 0.2 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = d ) # The linear regression model of D on A

#To simulate from the priors, we can use extract.prior and link as in the previous chapter. I’ll plot the lines over the range of 2 standard deviations for both the outcome and predictor. That’ll cover most of the possible range of both variables.
set.seed(10)
prior <- extract.prior( m5.1 )
mu <- link( m5.1 , post=prior , data=list( A=c(-2,2) ) )
plot( NULL , xlim=c(-2,2) , ylim=c(-2,2) )
for ( i in 1:50 ) lines( c(-2,2) ,
                         mu[i,] , col=col.alpha("black",0.4) ) #Plausibleregressionlinesimplied by the priors in m5.1. These are weakly infor- mative priors in that they allow some implusi- bly strong relationships but generally bound the lines to possible ranges of the variables.


#Now for the posterior predictions. The procedure is exactly like the examples from the previous chapter: link, then summarize with mean and PI, and then plot.
# compute percentile interval of mean
A_seq <- seq( from=-3 , to=3.2 , length.out=30 ) 
mu <- link( m5.1 , data=list(A=A_seq) )
mu.mean <- apply( mu , 2, mean )
mu.PI <- apply( mu , 2, PI )
# plot it all
plot( D ~ A , data=d , col=rangi2 )
lines( A_seq , mu.mean , lwd=2 )
shade( mu.PI , A_seq )  # Here we have plotted mean age marriage as as a function of divorce rate.

# Now we plot Marriage rate as a function of divorce rate:
m5.2 <- quap( 
  alist(
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bM * M ,
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = d )

set.seed(10)
prior <- extract.prior( m5.2 )
mu <- link( m5.2 , post=prior , data=list( M=c(-2,2) ) )
plot( NULL , xlim=c(-2,2) , ylim=c(-2,2) )
for ( i in 1:50 ) lines( c(-2,2) ,
                         mu[i,] , col=col.alpha("black",0.4) )

M_seq <- seq( from=-3 , to=3.2 , length.out=30 ) 
mu <- link( m5.2 , data=list(M=M_seq) )
mu.mean <- apply( mu , 2, mean )
mu.PI <- apply( mu , 2, PI )
plot( D ~ M , data=d , col=rangi2 )
lines( M_seq , mu.mean , lwd=2 )
shade( mu.PI , M_seq ) 




```

5.1.1. Think before you regress (DAG)
 - D = Divorce rate
 - M = Marriage rate
 - A = Median marriage age in each state

 - DAG: Graph of causal influence

```{r 5.1.2. Testable implications (DAG)}
# - Conditional independencies

# Check for correlations with cor()

pacman::p_load(dagitty)
#Here’s the code to define the second DAG and display the implied conditional independencies:
DMA_dag2 <- dagitty('dag{ D <- A -> M }')
impliedConditionalIndependencies( DMA_dag2 )
#The first DAG has no conditional independencies. You can define it and check with this:
DMA_dag1 <- dagitty('dag{ D <- A -> M -> D }') 
impliedConditionalIndependencies( DMA_dag1 ) #There are no conditional independencies, so there is no output to display.


#To test this implication, we need a statistical model that conditions on A, so we can see whether that renders D independent of M
```

5.1.3. Multiple regression notation

(1) Nominate the predictor variables you want in the linear model of the mean.
(2) For each predictor, make a parameter that will measure its conditional association with the outcome.
(3) Multiply the parameter by the variable and add that term to the linear model.

Marriage/Divorce example

Di ∼ Normal(μi, σ)            [probability of data]
μi = α + β_M*Mi + β_A*Ai      [linear model]
α ∼ Normal(0, 0.2)            [prior for α]
βM ∼ Normal(0, 0.5)           [prior for βM]
βA ∼ Normal(0, 0.5)           [prior for βA] 
σ ∼ Exponential(1)            [prior for σ]

M for marriage rate and A for age at marriage

```{r 5.1.4. Approximating the posterior}
# To fit this model to the divorce data, we just expand the linear model. Here’s the model definition again, with the code on the right-hand side:
#   Di ∼ Normal(μi, σ)      D ~ dnorm(mu,sigma)
#   μi =α+βMMi +βAAi        mu<-a+bM*M+bA*A 
#   α ∼ Normal(0, 0.2)      a ~ dnorm(0,0.2)
#   βM ∼ Normal(0, 0.5)     bM ~ dnorm(0,0.5)
#   βA ∼ Normal(0, 0.5)     bA ~ dnorm(0,0.5)
#   σ ∼ Exponential(1)      sigma ~ dexp(1)

#And here is the quap code to approximate the posterior distribution:
m5.3 <- quap( 
  alist(
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bM*M + bA*A , 
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = d ) 
precis( m5.3 )


#The posterior mean for marriage rate, bM, is now close to zero, with plenty of probability of both sides of zero. The posterior mean for age at marriage, bA, is essentially unchanged. 

#It will help to visualize the posterior distributions for all three models, focusing just on the slope parameters βA and βM:
pacman::p_load(rethinking)
plot( coeftab(m5.1, m5.2, m5.3), par=c("bA","bM") )  #I get an error here.


```

5.1.5. Plotting multivariate posteriors

(1) Predictor residual plots. These plots show the outcome against residual predictor values. They are useful for understanding the statistical model, but not much else. 

(2) Posterior prediction plots. These show model-based predictions against raw data, or otherwise display the error in prediction. They are tools for checking fit and assessing predictions. They are not causal tools.

(3) Counterfactual plots. These show the implied predictions for imaginary experiments. These plots allow you to explore the causal implications of manipulating one or more variables.

```{r 5.1.5.1. Plotting multivariate posteriors: Predictor residual plots}
#In our model of divorce rate, we have two predictors: (1) marriage rate M and (2) median age at marriage A. To compute predictor residuals for either, we just use the other predictor to model it.

#   Mi ∼ Normal(μi, σ) 
#   μi = α + βAi
#   α ∼ Normal(0, 0.2) 
#   β ∼ Normal(0, 0.5) 
#   σ ∼ Exponential(1)

m5.4 <- quap( 
  alist(
    M ~ dnorm( mu , sigma ) , 
    mu <- a + bAM * A ,
    a ~ dnorm( 0 , 0.2 ) , 
    bAM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = d )

#And then we compute the residuals by subtracting the observed marriage rate in each State from the predicted rate, based upon the model above:
mu <- link(m5.4)
mu_mean <- apply( mu , 2 , mean ) 
mu_resid <- d$M - mu_mean
#When a residual is positive, that means that the observed rate was in excess of what the model expects, given the median age at marriage in that State. When a residual is negative, that means the observed rate was below what the model expects.


# But how did he actually make the plots?
```

```{r 5.1.5.2. Plotting multivariate posteriors: Posterior residual plots}
#Let’s begin by simulating predictions, averaging over the posterior.

# call link without specifying new data so it uses original data
mu <- link( m5.3 )
# summarize samples across cases 
mu_mean <- apply( mu , 2 , mean ) 
mu_PI <- apply( mu , 2 , PI )
# simulate observations. again no new data, so uses original data 
D_sim <- sim( m5.3 , n=1e4 )
D_PI <- apply( D_sim , 2 , PI )

#plot predictions against observed. This code will do that, and then add a line to show perfect prediction and line segments for the confidence interval of each prediction:
plot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) , xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 )
#It’s easy to see from this arrangement of the sim- ulations that the model under-predicts for States with very high divorce rates while it over- predicts for States with very low divorce rates. That’s normal. This is what regression does—it is skeptical of extreme values, so it expects regression towards the mean.

#some States are very frustrating to the model, lying very far from the diagonal. I’ve labeled some points like this, including Idaho (ID) and Utah (UT), both of which have much lower divorce rates than the model expects them to have. The easiest way to label a few select points is to use identify:
identify( x=d$D , y=mu_mean , labels=d$Loc ) #This code must be run with the above plot code. Run outside of rmd chunk to get plot in active plot window, do you can click it.


```
#the following code goes with the chunk above^
plot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) , xlab="Observed divorce" , ylab="Predicted divorce" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 )
identify( x=d$D , y=mu_mean , labels=d$Loc )

```{r 5.1.5.3. Plotting multivariate posteriors: Counterfactual plots}
#  (1) Pick a variable to manipulate, the intervention variable.
#  (2) Define the range of values to set the intervention variable to.
#  (3) For each value of the intervention variable, and for each sample in posterior, use the causal model to simulate the values of other variables, including the outcome.


# To simulate from this, we need more than the DAG. We also need a set of functions that tell us how each variable is generated. For simplicity, we’ll use Gaussian distributions for each variable, just like in model m5.3. But model m5.3 ignored the assumption that A influences M. We didn’t need that to estimate A → D. But we do need it to predict the consequences of manipulating A, because some of the effect of A acts through M. To estimate the influence of A on M, all we need is to regress A on M. There are no other variables in the DAG creating an association between A and M. We can just add this regression to the quap model, running two regressions at the same time:
data(WaffleDivorce) 
d <- list()
d$A <- standardize( WaffleDivorce$MedianAgeMarriage )
d$D <- standardize( WaffleDivorce$Divorce )
d$M <- standardize( WaffleDivorce$Marriage )

m5.3_A <- quap( 
  alist(
    ## A -> D <- M
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bM*M + bA*A , 
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 ),
    ## A -> M
    M ~ dnorm( mu_M , sigma_M ), 
    mu_M <- aM + bAM*A,
    aM ~ dnorm( 0 , 0.2 ),
    bAM ~ dnorm( 0 , 0.5 ), 
    sigma_M ~ dexp( 1 )
) , data = d )
precis(m5.3_A) #You’ll see that M and A are strongly negatively associated. If we interpret this causally, it indicates that manipulating A reduces M.

#The goal is to simulate what would happen, if we manipulate A. So next we define a range of values for A.
A_seq <- seq( from=-2 , to=2 , length.out=30 ) #This defines a list of 30 imaginary interventions, ranging from 2 standard deviations below and 2 above the mean
 
#Now we can use sim, which you met in the previous chapter, to simulate observations from model m5.3_A. But this time we’ll tell it to simulate both M and D, in that order
# prep data
sim_dat <- data.frame( A=A_seq )
# simulate M and then D, using A_seq
s <- sim( m5.3_A , data=sim_dat , vars=c("M","D") )

#plot:
plot( sim_dat$A , colMeans(s$D) , ylim=c(-2,2) , type="l" ,
      xlab="manipulated A" , ylab="counterfactual D" )
shade( apply(s$D,2,PI) , sim_dat$A )
mtext( "Total counterfactual effect of A on D" ) #This predicted trend in D includes both paths: A → D and A → M → D. We found previously that M → D is very small, so the second path doesn’t contribute much to the trend.
#Look at the overthinking box for more detail (p.144)


#Of course these calculations also permit numerical summaries. For example, the ex- pected causal effect of increasing median age at marriage from 20 to 30 is:
# new data frame, standardized to mean 26.1 and std dev 1.24:
sim2_dat <- data.frame( A=(c(20,30)-26.1)/1.24 )
s2 <- sim( m5.3_A , data=sim2_dat , vars=c("M","D") )
mean( s2$D[,2] - s2$D[,1] ) #This is a huge effect of four and one half standard deviations, probably impossibly large.




#The arrow A → M is deleted, because if we control the values of M, then A no longer influ- ences it. It’s like a perfectly controlled experiment. Now we can modify the code above to simulate the counterfactual result of manipulating M. We’ll simulate a counterfactual for an average state, with A = 0, and see what changing M does.
sim_dat <- data.frame( M=seq(from=-2,to=2,length.out=30) , A=0 ) 
s <- sim( m5.3_A , data=sim_dat , vars="D" )

plot( sim_dat$M , colMeans(s) , ylim=c(-2,2) , type="l" ,
      xlab="manipulated M" , ylab="counterfactual D" )
shade( apply(s,2,PI) , sim_dat$M )
mtext( "Total counterfactual effect of M on D" ) #We only simulate D now — note the vars argument to sim() in the code above. We don’t simulate A, because M doesn’t influence it.


```

# Masked relationships
```{r 5.2. Masked relationship (part 1): Priors}
library(rethinking) 
data(milk)
d <- milk
str(d)
#You should see in the structure of the data frame that you have 29 rows for 8 variables. The variables we’ll consider for now are:
#   - kcal.per.g (kilocalories of energy per gram of milk)
#   - mass (average female body mass, in kilograms)
#   - neocortex.perc (percent of total brain mass that is neocortex mass)

# The question here is to what extent energy content of milk, measured here by kilocalories, is related to the percent of the brain mass that is neocortex.

#Standardizing the variables:
d$K <- standardize( d$kcal.per.g ) 
d$N <- standardize( d$neocortex.perc ) 
d$M <- standardize( log(d$mass) )

#The first model to consider is the simple bivariate regression between kilocalories and neocortex percent.
#   Ki ∼ Normal(μi, σ) 
#   μi =α+β_N*N_i
m5.5_draft <- quap( #!!! OBS: This code is supposed to give error
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bN*N ,
    a ~ dnorm( 0 , 1 ) ,
    N ~ dnorm( 0 , 1 ) , 
    sigma ~ dexp( 1 )
) , data=d ) #I don't get exactly the same error that he gets in the book
d$neocortex.perc #We have a bunch of NAs. We need to drop all of the NAs:
dcc <- d[ complete.cases(d$K,d$N,d$M) , ]
#Now, let's try again:
m5.5_draft <- quap( 
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bN*N ,
    a ~ dnorm( 0 , 1 ) ,
    bN ~ dnorm( 0 , 1 ) , sigma ~ dexp( 1 )
) , data=dcc )

#Priors: To simulate and plot 50 prior regression lines:
prior <- extract.prior( m5.5_draft )
xseq <- c(-2,2)
mu <- link( m5.5_draft , post=prior , data=list(N=xseq) )
plot( NULL , xlim=xseq , ylim=xseq )
for ( i in 1:50 ) lines( xseq , mu[i,], 
                         col=col.alpha("black",0.3) ) #These lines are crazy. As in previous examples, we can do better by both tightening the α prior so that it sticks closer to zero. With two standardized variables, when predictor is zero, the expected value of the outcome should also be zero. And the slope βN needs to be a bit tighter as well, so that it doesn’t regularly produce impossibly strong relationships. Here’s an attempt:
m5.5 <- quap( 
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bN*N ,
    a ~ dnorm( 0 , 0.2 ) , 
    bN ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data=dcc )
prior <- extract.prior( m5.5 )
xseq <- c(-2,2)
mu <- link( m5.5 , post=prior , data=list(N=xseq) )
plot( NULL , xlim=xseq , ylim=xseq )
for ( i in 1:50 ) lines( xseq , mu[i,], 
                         col=col.alpha("black",0.3) ) #These are still very vague priors, but at least the lines stay within the high probability region of the observable data.
```


```{r 5.2. Masked relationship (continued): Posteriors}
#Now, we look at the posterior:
precis(m5.5) #From this summary, you can possibly see that this is neither a strong nor very precise asso- ciation. The standard deviation is almost twice the posterior mean.
#plot:
xseq <- seq( from=min(dcc$N)-0.15 , to=max(dcc$N)+0.15 , length.out=30 ) 
mu <- link( m5.5 , data=list(N=xseq) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( K ~ N , data=dcc ) 
lines( xseq , mu_mean , lwd=2 ) 
shade( mu_PI , xseq )
#The posterior mean line is weakly positive, but it is highly imprecise. A lot of mildly positive and negative slopes are plausible, given this model and these data.



#Now consider another predictor variable, adult female body mass, mass in the data frame. Let’s use the logarithm of mass, log(mass), as a predictor as well. 
#    - Why the logarithm of mass instead of the raw mass in kilograms? It is often true that scaling measurements like body mass are related by magnitudes to other variables. Taking the log of a measure trans- lates the measure into magnitudes. So by using the logarithm of body mass here, we’re saying that we suspect that the magnitude of a mother’s body mass is related to milk energy, in a linear fashion.

#Now we construct a similar model, but consider the bivariate relationship between kilo- calories and body mass (What?).
m5.6 <- quap( 
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bM*M ,
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data=dcc ) 
precis(m5.6) #Log-mass is negatively associated with kilocalories. This association does seem stronger than that of neocortex percent, although in the opposite direction. It is quite uncertain though, with a wide compatibility interval that is consistent with a wide range of both weak and stronger relationships.



# Now let’s see what happens when we add both predictor variables at the same time to the regression. This is the multivariate model, in math form:
#     Ki ∼ Normal(μi, σ)
#     μi = α + β_N*N_i + β_M*M_i
#     α ∼ Normal(0, 0.2) 
#     βN ∼ Normal(0, 0.5) 
#     βM ∼ Normal(0, 0.5) 
#     σ ∼ Exponential(1)
m5.7 <- quap( 
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bN*N + bM*M , 
    a ~ dnorm( 0 , 0.2 ) , 
    bN ~ dnorm( 0 , 0.5 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data=dcc ) 
precis(m5.7)

plot( coeftab( m5.5 , m5.6 , m5.7 ) , pars=c("bM","bN") ) #(!!! not working)

pairs( ~K + M + N , dcc )


# Moving back to counterfactual plots again:
xseq <- seq( from=min(dcc$M)-0.15 , to=max(dcc$M)+0.15 , length.out=30 ) 
mu <- link( m5.7 , data=data.frame( M=xseq , N=0 ) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( NULL , xlim=range(dcc$M) , ylim=range(dcc$K) ) 
lines( xseq , mu_mean , lwd=2 )
shade( mu_PI , xseq )
```

#Categorical variables
```{r 5.3.1. Categorical variables: Binary categories}
data(Howell1) 
d <- Howell1 
str(d)
#The prior distributions for μ for females and males are:
mu_female <- rnorm(1e4,178,20)
mu_male <- rnorm(1e4,178,20) + rnorm(1e4,0,10) 
precis( data.frame( mu_female , mu_male ) ) #The prior for males is wider, because it uses both parameters. While in a regression this simple, these priors will wash out very quickly, in general we should be careful.

#Another approach: Index variable - solves the prior problem
d$sex <- ifelse( d$male==1 , 2 , 1 ) 
str( d$sex )#Now “1” means female and “2” means male
#What this does is create a list of α parameters, one for each unique value in the index variable. So in this case we end up with two α parameters, named α1 and α2.

#Now the same prior can be assigned to each, corresponding to the notion that all the categories are the same, prior to the data. Neither category has more prior uncertainty than the other.

#Approximating posterior:
m5.8 <- quap( 
  alist(
    height ~ dnorm( mu , sigma ) , 
    mu <- a[sex] ,
    a[sex] ~ dnorm( 178 , 20 ) , 
    sigma ~ dunif( 0 , 50 )
) , data=d ) 
precis( m5.8 , depth=2 ) #Note the depth=2 that I added to precis. This tells it to show any vector parameters, like our new a vector. 
#Interpretation: They are the expected heights in each category.

#differences between categories:
post <- extract.samples(m5.8) 
post$diff_fm <- post$a[,1] - post$a[,2] 
precis( post , depth=2 ) #Our calculation appears at the bottom, as a new parameter in the posterior. This is the ex- pected difference between a female and male in the sample. This kind of calculation is called a contrast.

```

```{r 5.3.1. Categorical variables: Many categories}
#We’re interested now in the clade variable, which encodes the broad taxonomic membership of each species
data(milk)
d <- milk 
levels(d$clade)
#We want an index value for each of these four categories
d$clade_id <- as.integer( d$clade )


d$K <- standardize( d$kcal.per.g ) 
m5.9 <- quap(
  alist(
    K ~ dnorm( mu , sigma ),
    mu <- a[clade_id],
    a[clade_id] ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
) , data=d )
labels <- paste( "a[" , 1:4 , "]:" , levels(d$clade) , sep="" ) 
plot( precis( m5.9 , depth=2 , pars="a" ) , labels=labels , xlab="expected kcal (std)" )   #This gives an error !!!



#If you have another kind of categorical variable that you’d like to add to the model, the approach is just the same. For example, let’s randomly assign these primates to some made up categories: [1] Gryffindor, [2] Hufflepuff, [3] Ravenclaw, and [4] Slytherin:
set.seed(63)
d$house <- sample( rep(1:4,each=8) , size=nrow(d) )

m5.10 <- quap( 
  alist(
    K ~ dnorm( mu , sigma ),
    mu <- a[clade_id] + h[house], 
    a[clade_id] ~ dnorm( 0 , 0.5 ), 
    h[house] ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
) , data=d )


```



