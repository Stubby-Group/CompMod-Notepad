---
title: "W2_Notes"
output: html_document
---

```{r setup, include=FALSE}
library(rethinking)

```

### Chapter 3 ###

```{r Chapter 3}
# Suppose there is a blood test that correctly detects vampirism 95% of the time. In more precise and mathematical notation, Pr(positive test result|vampire) = 0.95. One percent of the time, it incorrectly diagnoses normal people as vampires, Pr(positive test result|mortal) = 0.01. The final bit of information we are told is that vampires are rather rare, being only 0.1% of the population, implying Pr(vampire) = 0.001. 

#Suppose now that someone tests positive for vampirism. What’s the probability that he or she is a bloodsucking immortal?

Pr_Positive_Vampire <- 0.95
Pr_Positive_Mortal <- 0.01
Pr_Vampire <- 0.001
Pr_Positive <- Pr_Positive_Vampire * Pr_Vampire +
  Pr_Positive_Mortal * ( 1 - Pr_Vampire )
Pr_Vampire_Positive <- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive
Pr_Vampire_Positive #That corresponds to an 8.7% chance that the suspect is actually a vampire.



# Better way to put it (you can try to "translate tasks like this, if you don't understand them): 
  # (1) In a population of 100,000 people, 100 of them are vampires.
  # (2) Of the 100 who are vampires, 95 of them will test positive for vampirism. 
  # (3) Of the 99,900 mortals, 999 of them will test positive for vampirism.
```

```{r 3.1. Sampling a grid-approximate posterior}
# Before beginning to work with samples, we need to generate them. Here’s a reminder for how to compute the posterior for the globe tossing model, using grid approximation. Remember, the posterior here means the probability of p conditional on the data.
p_grid <- seq( from=0 , to=1 , length.out=1000 ) 
prob_p <- rep( 1 , 1000 )
prob_data <- dbinom( 6 , size=9 , prob=p_grid ) 
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)

# Now we wish to draw 10,000 samples from this posterior. The individual values of p will appear in our samples in proportion to the posterior plausibility of each value.
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )

plot(samples) #10000 random samples are shown sequentially

dens(samples) 

# Posterior distribution
```

```{r 3.2. Sampling to summarize}
# Common questions (depending on purpose) include:
# • How much posterior probability lies below some parameter value?
# • How much posterior probability lies between two parameter values?
# • Which parameter value marks the lower 5% of the posterior probability?
# • Which range of parameter values contains 90% of the posterior probability? 
# • Which parameter value has highest posterior probability?

# These simple questions can be usefully divided into questions about 
#  (1) intervals of defined boundaries, 
#  (2) questions about intervals of defined probability mass, and 
#  (3) questions about point estimates.



### (1) intervals of defined boundaries ###
# What is the posterior probability that the proportion of water is less than 0.5?
sum(posterior[ p_grid < 0.5 ]) # add up posterior probability where p < 0.5
# or: find the frequency of parameter values below 0.5:
sum(samples < 0.5) / 10000 # Diff result die to diff samples

# how much posterior probability lies between 0.5 and 0.75:
sum( samples > 0.5 & samples < 0.75 ) / 1e4
 


### (2) intervals of defined mass ### 
# Confidence / credibility / compatibility intervals. What the interval indicates is a range of parameter values compatible with the model and data.
#Suppose for example you want to know the boundaries of the lower 80% posterior probabil- ity. You know this interval starts at p = 0. To find out where it stops, think of the samples as data and ask where the 80th percentile lies:
quantile(samples, 0.8)
# Similarly, the middle 80% interval lies between the 10th percentile and the 90th percentile.
quantile( samples , c( 0.1 , 0.9 ) )

#Percentile intervals (!!! I don't get it)
p_grid <- seq( from=0 , to=1 , length.out=1000 ) 
prior <- rep(1,1000)
likelihood <- dbinom( 3 , size=3 , prob=p_grid ) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )
PI( samples , prob=0.5 )
# HPDI = Highest posterior density interval = the narrowest interval containing the specified probability mass. HPDI and PI are very similar.
HPDI( samples , prob=0.5 )


### (3) Point estimate ### see the book
```

```{r 3.3.1 Sampling to simulate prediction - Dummy data}

###### 3.3.1: Dummy data (simulated predictions) ######

# Suppose N = 2, two tosses of the globe. Then there are only three possible observations: 0 water, 1 water, 2 water. You can quickly compute the probability of each, for any given value of p. Let’s use p = 0.7, which is just about the true proportion of water on the Earth:
dbinom( 0:2 , size=2 , prob=0.7 ) #This means that there’s a 9% chance of observing w = 0, a 42% chance of w = 1, and a 49% chance of w = 2.

#Now we’re going to simulate observations, using these probabilities. A single dummy data observation of W can be sampled with:
rbinom( 1 , size=2 , prob=0.7 ) #That 1 means “1 water in 2 tosses.” The “r” in rbinom stands for “random.” 
#It can alsogenerate more than one simulation at a time. A set of 10 simulations can be made by:
rbinom( 10 , size=2 , prob=0.7 )
#Let’s generate 100,000 dummy observations, just to verify that each value (0, 1, or 2) appears in proportion to its likelihood:
dummy_w <- rbinom( 1e5 , size=2 , prob=0.7 ) 
table(dummy_w)/1e5

# Only two tosses of the globe isn’t much of a sample, though. So now let’s simulate the same sample size as before, 9 tosses.
dummy_w <- rbinom( 1e5 , size=9 , prob=0.7 ) 
simplehist( dummy_w , xlab="dummy water count" )
```

```{r 3.3.2 Sampling to simulate prediction - Model checking}
# Ensuring that the model fitting worked correctly and evaluating the adequacy of a model for some purpose.


# To simulate predicted observations for a single value of p, say p = 0.6, you can use rbinom to generate random binomial samples:
 w <- rbinom( 1e4 , size=9 , prob=0.6 ) #This generates 10,000 (1e4) simulated predictions of 9 globe tosses (size=9), assuming p = 0.6.
simplehist(w)

#All you need to propagate parameter uncertainty into these predictions is replace the value 0.6 with samples from the posterior:
 w <- rbinom( 1e4 , size=9 , prob=samples )
```

### Chapter 4 ###

```{r 4.1. Why normal distributions are normal}
### 4.1.1. Normal by addition
# Football field / coint toss with 1,000 friends example: "To simulate this, we generate for each person a list of 16 random numbers between −1 and 1. These are the individual steps. Then we add these steps together to get the position after 16 steps. Then we need to replicate this procedure 1000 times."
pos <- replicate( 1000 , sum( runif(16,-1,1) ) )

hist(pos)
plot(density(pos))



### 4.1.2. Normal by multiplication
#we can sample a random growth rate for this example with this line of code:
prod( 1 + runif(12,0,0.1) ) # This code just samples 12 random numbers between 1.0 and 1.1, each representing a pro- portional increase in growth. Thus 1.0 means no additional growth and 1.1 means a 10% increase. The product of all 12 is computed and returned as output. Now what distribution do you think these random products will take? Let’s generate 10,000 of them and see:
growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) ) 
dens( growth , norm.comp=TRUE )

#small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions.
big <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) ) 
small <- replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )
dens( big , norm.comp=TRUE )
dens( small , norm.comp=TRUE )



### 4.1.3. Normal by log-multiplication
#Large deviates that are multi- plied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale. For example:
log.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )
dens(log.big, norm.comp=TRUE )
```

## OBS, you skipped pages 75-76, because they are boring and irrelevant. Might wanna go back there someday.

```{r 4.2. A language for describing models}
## 4.2.1. Re-describing the globe tossing model
#   W = observed count of water
#   N = the total number of tosses
#   p = the proportion of water on the globe
#The count W is distributed binomially with sample size N and probability p (this line defines the likelihood function). The prior for p is assumed to be uniform between zero and one (this line defines the priors).



#Okay... nothing actually happens here, sooo
```

```{r 4.3 Gaussian model of height}
# Building a linear regression model (kinda).
```

