---
title: "W2_Notes"
output: html_document
---
# Definitions and things to remember:
 - Alpha, Beta, and Sigma are PARAMETERS (unobserved variables). 
 - PRIORS are theand the distributions of the PARAMETERS (pp.94).



```{r setup, include=FALSE}
library(rethinking)

```

### Chapter 3 ###

```{r Chapter 3}
# Suppose there is a blood test that correctly detects vampirism 95% of the time. In more precise and mathematical notation, Pr(positive test result|vampire) = 0.95. One percent of the time, it incorrectly diagnoses normal people as vampires, Pr(positive test result|mortal) = 0.01. The final bit of information we are told is that vampires are rather rare, being only 0.1% of the population, implying Pr(vampire) = 0.001. 

#Suppose now that someone tests positive for vampirism. What’s the probability that he or she is a bloodsucking immortal?

Pr_Positive_Vampire <- 0.95
Pr_Positive_Mortal <- 0.01
Pr_Vampire <- 0.001
Pr_Positive <- Pr_Positive_Vampire * Pr_Vampire +
  Pr_Positive_Mortal * ( 1 - Pr_Vampire )
Pr_Vampire_Positive <- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive
Pr_Vampire_Positive #That corresponds to an 8.7% chance that the suspect is actually a vampire.



# Better way to put it (you can try to "translate tasks like this, if you don't understand them): 
  # (1) In a population of 100,000 people, 100 of them are vampires.
  # (2) Of the 100 who are vampires, 95 of them will test positive for vampirism. 
  # (3) Of the 99,900 mortals, 999 of them will test positive for vampirism.
```

```{r 3.1. Sampling a grid-approximate posterior}
# Before beginning to work with samples, we need to generate them. Here’s a reminder for how to compute the posterior for the globe tossing model, using grid approximation. Remember, the posterior here means the probability of p conditional on the data.
p_grid <- seq( from=0 , to=1 , length.out=1000 ) 
prob_p <- rep( 1 , 1000 )
prob_data <- dbinom( 6 , size=9 , prob=p_grid ) 
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)

# Now we wish to draw 10,000 samples from this posterior. The individual values of p will appear in our samples in proportion to the posterior plausibility of each value.
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )

plot(samples) #10000 random samples are shown sequentially

dens(samples) 

# Posterior distribution
```

```{r 3.2. Sampling to summarize}
# Common questions (depending on purpose) include:
# • How much posterior probability lies below some parameter value?
# • How much posterior probability lies between two parameter values?
# • Which parameter value marks the lower 5% of the posterior probability?
# • Which range of parameter values contains 90% of the posterior probability? 
# • Which parameter value has highest posterior probability?

# These simple questions can be usefully divided into questions about 
#  (1) intervals of defined boundaries, 
#  (2) questions about intervals of defined probability mass, and 
#  (3) questions about point estimates.



### (1) intervals of defined boundaries ###
# What is the posterior probability that the proportion of water is less than 0.5?
sum(posterior[ p_grid < 0.5 ]) # add up posterior probability where p < 0.5
# or: find the frequency of parameter values below 0.5:
sum(samples < 0.5) / 10000 # Diff result die to diff samples

# how much posterior probability lies between 0.5 and 0.75:
sum( samples > 0.5 & samples < 0.75 ) / 1e4
 


### (2) intervals of defined mass ### 
# Confidence / credibility / compatibility intervals. What the interval indicates is a range of parameter values compatible with the model and data.
#Suppose for example you want to know the boundaries of the lower 80% posterior probabil- ity. You know this interval starts at p = 0. To find out where it stops, think of the samples as data and ask where the 80th percentile lies:
quantile(samples, 0.8)
# Similarly, the middle 80% interval lies between the 10th percentile and the 90th percentile.
quantile( samples , c( 0.1 , 0.9 ) )

#Percentile intervals (!!! I don't get it)
p_grid <- seq( from=0 , to=1 , length.out=1000 ) 
prior <- rep(1,1000)
likelihood <- dbinom( 3 , size=3 , prob=p_grid ) 
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )
PI( samples , prob=0.5 )
# HPDI = Highest posterior density interval = the narrowest interval containing the specified probability mass. HPDI and PI are very similar.
HPDI( samples , prob=0.5 )


### (3) Point estimate ### see the book
```

```{r 3.3.1 Sampling to simulate prediction - Dummy data}

###### 3.3.1: Dummy data (simulated predictions) ######

# Suppose N = 2, two tosses of the globe. Then there are only three possible observations: 0 water, 1 water, 2 water. You can quickly compute the probability of each, for any given value of p. Let’s use p = 0.7, which is just about the true proportion of water on the Earth:
dbinom( 0:2 , size=2 , prob=0.7 ) #This means that there’s a 9% chance of observing w = 0, a 42% chance of w = 1, and a 49% chance of w = 2.

#Now we’re going to simulate observations, using these probabilities. A single dummy data observation of W can be sampled with:
rbinom( 1 , size=2 , prob=0.7 ) #That 1 means “1 water in 2 tosses.” The “r” in rbinom stands for “random.” 
#It can alsogenerate more than one simulation at a time. A set of 10 simulations can be made by:
rbinom( 10 , size=2 , prob=0.7 )
#Let’s generate 100,000 dummy observations, just to verify that each value (0, 1, or 2) appears in proportion to its likelihood:
dummy_w <- rbinom( 1e5 , size=2 , prob=0.7 ) 
table(dummy_w)/1e5

# Only two tosses of the globe isn’t much of a sample, though. So now let’s simulate the same sample size as before, 9 tosses.
dummy_w <- rbinom( 1e5 , size=9 , prob=0.7 ) 
simplehist( dummy_w , xlab="dummy water count" )
```

```{r 3.3.2 Sampling to simulate prediction - Model checking}
# Ensuring that the model fitting worked correctly and evaluating the adequacy of a model for some purpose.


# To simulate predicted observations for a single value of p, say p = 0.6, you can use rbinom to generate random binomial samples:
 w <- rbinom( 1e4 , size=9 , prob=0.6 ) #This generates 10,000 (1e4) simulated predictions of 9 globe tosses (size=9), assuming p = 0.6.
simplehist(w)

#All you need to propagate parameter uncertainty into these predictions is replace the value 0.6 with samples from the posterior:
 w <- rbinom( 1e4 , size=9 , prob=samples )
```

### Chapter 4 ###

```{r 4.1. Why normal distributions are normal}
### 4.1.1. Normal by addition
# Football field / coint toss with 1,000 friends example: "To simulate this, we generate for each person a list of 16 random numbers between −1 and 1. These are the individual steps. Then we add these steps together to get the position after 16 steps. Then we need to replicate this procedure 1000 times."
pos <- replicate( 1000 , sum( runif(16,-1,1) ) )

hist(pos)
plot(density(pos))



### 4.1.2. Normal by multiplication
#we can sample a random growth rate for this example with this line of code:
prod( 1 + runif(12,0,0.1) ) # This code just samples 12 random numbers between 1.0 and 1.1, each representing a pro- portional increase in growth. Thus 1.0 means no additional growth and 1.1 means a 10% increase. The product of all 12 is computed and returned as output. Now what distribution do you think these random products will take? Let’s generate 10,000 of them and see:
growth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) ) 
dens( growth , norm.comp=TRUE )

#small effects that multiply together are approximately additive, and so they also tend to stabilize on Gaussian distributions.
big <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) ) 
small <- replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )
dens( big , norm.comp=TRUE )
dens( small , norm.comp=TRUE )



### 4.1.3. Normal by log-multiplication
#Large deviates that are multi- plied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale. For example:
log.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )
dens(log.big, norm.comp=TRUE )
```

## OBS, you skipped pages 75-76, because they are boring and irrelevant. Might wanna go back there someday.

```{r 4.2. A language for describing models}
## 4.2.1. Re-describing the globe tossing model
#   W = observed count of water
#   N = the total number of tosses
#   p = the proportion of water on the globe
#The count W is distributed binomially with sample size N and probability p (this line defines the likelihood function). The prior for p is assumed to be uniform between zero and one (this line defines the priors).



#Okay... nothing actually happens here, sooo
```

```{r 4.3.1. Gaussian model of height}
# Building a linear regression model (kinda). Mean and standard deviation describe the distribution's shape.

# Loading example data:
data(Howell1)
d <- Howell1

# Summarizing data:
precis( d ) #If you cannot see the histograms on your system, use instead: precis(d,hist=FALSE)

# All we want for now are heights of adults in the sample:
 d2 <- d[ d$age >= 18 , ]
 
 dens(d2$height)
```

```{r 4.3.2. Gaussian model of height: The model}

#    h_i ∼ Normal(μ, σ)         [likelihood]
#    μ ∼ Normal(178, 20)        [μ prior]
#    σ ∼ Uniform(0, 50)         [σ prior]
#The prior for μ is a broad Gaussian prior, centered on 178 cm, with 95% of probability between 178 ± 40 cm. Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218 cm encom- passes a huge range of plausible mean heights for human populations. So domain-specific information has gone into this prior. Everyone knows something about human height and can set a reasonable and vague prior of this kind.

#plot priors:
curve( dnorm( x , 178 , 20 ) , from=100 , to=250 )

#The σ prior is a truly flat prior, a uniform one, that functions just to constrain σ to have positive probability between zero and 50 cm:
curve( dunif( x , 0 , 50 ) , from=-10 , to=60 )



#it’ll help to see what these priors imply about the distribution of individual heights. The prior predictive simulation is an essential part of your modeling. (...) Every posterior is also potentially a prior for a subsequent analysis, so you can process priors just like posteriors.
sample_mu <- rnorm( 1e4 , 178 , 20 )
sample_sigma <- runif( 1e4 , 0 , 50 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) 
dens( prior_h )

#Let’s use simulation again to see the implied heights:
sample_mu <- rnorm( 1e4 , 178 , 100 )
prior_h <- rnorm( 1e4 , sample_mu , sample_sigma ) 
dens( prior_h )
```

```{r 4.3.3. Gaussian model of height: Grid approximation of the posterior distribution}
# The following is manual and is shown for conceptual rather than practical reasons.
mu.list <- seq( from=150, to=160 , length.out=100 ) 
sigma.list <- seq( from=7 , to=9 , length.out=100 ) 
post <- expand.grid( mu=mu.list , sigma=sigma.list ) 
post$LL <- sapply( 1:nrow(post) , function(i) sum(
  dnorm( d2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) ) 
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) + 
  dunif( post$sigma , 0 , 50 , TRUE ) 
post$prob <- exp( post$prod - max(post$prod) )

#Contour plot:
contour_xyz( post$mu , post$sigma , post$prob )

#Heat map:
image_xyz( post$mu , post$sigma , post$prob )
```

```{r 4.3.4. Gaussian model of height: Sampling from the posterior}
# since there are two parameters, and we want to sample combinations of them, we first randomly sample row numbers in post in proportion to the values in post$prob. Then we pull out the parameter values on those randomly sampled rows:
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE , prob=post$prob )
sample.mu <- post$mu[ sample.rows ] 
sample.sigma <- post$sigma[ sample.rows ]

plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )

#Now that you have these samples, you can describe the distribution of confidence in each combination of μ and σ by summarizing the samples.
#For example, to characterize the shapes of the marginal (“marginal” here means “averaging over the other parameters.”) posterior densities of μ and σ, all we need to do is:
dens( sample.mu ) 
dens( sample.sigma )
# To summarize the widths of these densities with posterior compatibility intervals:
PI( sample.mu ) 
PI( sample.sigma )

#Or:
mean()
median()
quantile()



# Overthinking: Repeating with only a fraction of original data (compressed code):
d3 <- sample( d2$height , size=20 )

mu.list <- seq( from=150, to=170 , length.out=200 ) 
sigma.list <- seq( from=4 , to=20 , length.out=200 ) 
post2 <- expand.grid( mu=mu.list , sigma=sigma.list ) 
post2$LL <- sapply( 1:nrow(post2) , function(i)
  sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] , log=TRUE ) ) )
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
  dunif( post2$sigma , 0 , 50 , TRUE )
post2$prob <- exp( post2$prod - max(post2$prod) )
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob )
sample2.mu <- post2$mu[ sample2.rows ] 
sample2.sigma <- post2$sigma[ sample2.rows ] 
plot( sample2.mu , sample2.sigma , cex=0.5 , 
      col=col.alpha(rangi2,0.1) , 
      xlab="mu" , ylab="sigma" , pch=16 )

# You should also inspect the marginal posterior density for σ, averaging over μ, produced with:
dens( sample2.sigma , norm.comp=TRUE )
```

```{r 4.3.5. Gaussian model of height: Finding the posterior distribution with quap}
#Quadratic approximation
# quap-function: "The quap function works by using the model definition you were introduced to ear- lier in this chapter. Each line in the definition has a corresponding definition in the form of R code. The engine inside quap then uses these definitions to define the posterior probability at each combination of parameter values. Then it can climb the posterior distribution and find the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce an approximation of the posterior distribution"

#(Re)loading example data:
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]

# List of (something)
flist <- alist(
  height ~ dnorm( mu , sigma ),
  mu ~ dnorm( 178 , 20 ), 
  sigma ~ dunif( 0 , 50 )
)

#Fit the model to the data in the data frame d2 with:
m4.1 <- quap( flist , data=d2 )

#Posterior distribution:
precis( m4.1 )

# Interpretation: "This means the plausibility of each value of μ, after averaging over the plausibilities of each value of σ, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4. The 5.5% and 94.5% quantiles are percentile interval boundaries, corresponding to an 89% compatibility interval. Why 89%? It’s just the default. It displays a quite wide interval, so it shows a high-probability range of parameter values. If you want another interval, such as the conventional and mindless 95%, you can use precis(m4.1,prob=0.95). But I don’t recommend 95% intervals, because readers will have a hard time not viewing them as significance tests."
```

```{r 4.3.6. Gaussian model of height: Sampling from a quap}
#Variance-covariance matrix: A list of means and a matrix of variances and covariances are sufficient to describe a multi-dimensional Gaussian distribution. To see this matrix of variances and covariances, for model m4.1, use:
vcov( m4.1 ) #it tells us how each parameter relates to every other param- eter in the posterior distribution.

#A variance-covariance matrix can be factored into two elements: (1) a vector of variances for the parameters and (2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others:
diag( vcov( m4.1 ) ) #List of variances
cov2cor( vcov( m4.1 ) ) # Correlation matrix. Interpretation: The entries that aren't 1 are very close to zero in this (typical) example. This indicates that learning μ tells us nothing about σ and likewise that learning σ tells us nothing about μ.

# Sampeling:
post <- extract.samples( m4.1 , n=1e4 )
head(post) #Each value is a sample from the posterior, so the mean and standard deviation of each column will be very close to the MAP values from before. You can confirm this by summarizing the samples:
precis(post)

#Interpretation: Compare these values to the output from precis(m4.1). And you can use plot(post) to see how much they resemble the samples from the grid approximation in Figure 4.4 (page 86).

```

```{r 4.4. Linear prediction}
#plot adult height and weight against one another:
data(Howell1); d <- Howell1; d2 <- d[ d$age >= 18 , ] 
plot( d2$height ~ d2$weight )
```

```{r 4.4.1. Linear prediction: The linear model strategy}
# About the prior of Beta: Simulating the prior predictive distribution

#The goal is to simulate heights from the model, using only the priors. First, let’s consider a range of weight values to simulate over. The range of observed weights will do fine. Then we need to simulate a bunch of lines, the lines implied by the priors for α and β:
set.seed(2971)
N <- 100 # 100 lines 
a <- rnorm( N , 178 , 20 )
b <- rnorm( N , 0 , 10 )
#Now we have 100 pairs of α and β values. Now to plot the lines:
plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , 
      xlab="weight" , ylab="height" )
abline( h=0 , lty=2 )
abline( h=272 , lty=1 , lwd=0.5 )
mtext( "b ~ dnorm(0,10)" )
xbar <- mean(d2$weight)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar), 
                        from=min(d2$weight) , to=max(d2$weight) , add=TRUE, 
                        col=col.alpha("black",0.2) )
# Interpretation: "The pattern doesn’t look like any human population at all. It essentially says that the relationship between weight and height could be absurdly positive or negative. Before we’ve even seen the data, this is a bad model.

# Here is how to do better: We know that average height increases with average weight, at least up to a point. Let’s try restricting it to positive values. The easiest way to do this is to define the prior as Log-Normal instead. Defining β as Log-Normal(0,1) means to claim that the logarithm of β has a Normal(0,1) distribution.
b <- rlnorm( 1e4 , 0 , 1 )
dens( b , xlim=c(0,5) , adj=0.1 )
#Do the prior predictive simulation again, now with the Log-Normal prior:
set.seed(2971)
N <- 100 # 100 lines 
a <- rnorm( N , 178 , 20 )
b <- rlnorm( N , 0 , 1 )
#Now try to plot again. This is much more sensible. There is still a rare impossible relationship. But nearly all lines in the joint prior for α and β are now within human reason.
```

```{r 4.4.1. Linear prediction: Finding the posterior distribution}

```

